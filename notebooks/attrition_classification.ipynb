{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3edce18f-86f0-465e-9ead-0dac6a6bf29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995e9f5c-c143-473e-96a0-e7bcce14f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666fa22a-514c-40c2-9ddb-9f335e76663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, recall_score, accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import joblib\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ed1b0-c299-4d1f-98df-66218a4e9528",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../src\")\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78472d5c-3ad1-43db-861d-ff7dcaebca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6563ae2c-7bb8-4ed8-8362-6405c7014baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372552a3-e75b-4881-8190-12a6beb51e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition = pd.read_csv(\"../cortAIx Factory Data Science Technical Test project #1.csv\")\n",
    "df_attrition.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e038a408-f8a6-4f83-bc00-65f1fbc587e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition=df_attrition.iloc[:, 2:]\n",
    "df_attrition.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82c590-acdb-4d40-b3b2-6b7b0cad3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93337023-c25d-469d-aea9-1d7a6e54f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba11626-644b-4bab-bbec-b5f414a64908",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e8223-54d7-4b4a-8b08-bd4010044a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb2a0f1-7578-42ed-b9e2-a2dfa4028b93",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95604f9-52b4-4eac-98d3-b34a3a4c8f16",
   "metadata": {},
   "source": [
    "## Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7e860-9495-464d-a683-a5a6f890dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition[\"Gender\"].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14af411-e73a-401f-b90d-608a287b7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition[\"Department\"].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff134ca-a96f-4ee5-9e68-6f01c707cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition[\"Job_Title\"].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a5846-f6cb-4af5-9963-588e468448bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition[\"Office_Localisation\"].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd1f03-e14b-4cf8-bc8c-ad2096dc03bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition[\"Promotion_Last_5Years\"].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91637b9-5301-42de-a7c7-4bfaad84aaec",
   "metadata": {},
   "source": [
    "Most categorical data are uniformly distributed, with some of the following issues:\n",
    "- Missing values in Department\n",
    "- Missing values in Job_Title\n",
    "- Outliers in Job_Title (values 1 and 2)\n",
    "- Outliers in Promotion_Last_5Years (values 45124 and 32770)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc288c9b-c482-4317-b004-106d0708cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_categorical = (\n",
    "    df_attrition[\"Department\"].isnull() |\n",
    "    df_attrition[\"Job_Title\"].isnull() |\n",
    "    df_attrition[\"Job_Title\"].isin([\"1\", \"2\"]) |\n",
    "    df_attrition[\"Promotion_Last_5Years\"].isin([45124, 32770])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ecb04-ac6a-4288-bf95-57023b912a62",
   "metadata": {},
   "source": [
    "## Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be053025-d995-4855-a218-17a04e9d8286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check for missing values\n",
    "df_attrition[[\"Age\", \"Years_at_Company\",\n",
    "             \"Satisfaction_Level\", \"Average_Monthly_Hours\",\n",
    "             \"Salary\"]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ea8e0-1f1a-432a-a4f5-214ba1edcc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check for not consistent data\n",
    "df_attrition[df_attrition[\"Age\"] < df_attrition[\"Years_at_Company\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9251c2b-1ade-4b8b-b0f0-da42711c0d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also check for 0 salaries to look for outliers\n",
    "df_attrition[df_attrition[\"Salary\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5f367-214d-407b-aa82-9d0e69c85634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check for outliers using boxplot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 5))\n",
    "axes[1, 2].remove()\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate([\"Age\", \"Years_at_Company\", \"Satisfaction_Level\",\n",
    "                        \"Average_Monthly_Hours\", \"Salary\"]):\n",
    "    plt.sca(axes[i])\n",
    "    sns.boxplot(y=df_attrition[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f3b7b-766b-4a3c-93eb-126560cbf376",
   "metadata": {},
   "source": [
    "- Variables Years_at_Company and Satisfaction_Level present some outliers as seen in the boxplots respectively for over 90 and over 150\n",
    "- Salaries equal to 0 are for employees being in the company for more than 3 years, which is weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb021ca-1666-459e-828c-940f7f66d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_numerical = (\n",
    "    df_attrition[\"Age\"].isnull() |\n",
    "    df_attrition[\"Years_at_Company\"].isnull() |\n",
    "    (df_attrition[\"Age\"] < df_attrition[\"Years_at_Company\"]) |\n",
    "    (df_attrition[\"Years_at_Company\"] > 90) |\n",
    "    (df_attrition[\"Satisfaction_Level\"] > 150) |\n",
    "    (df_attrition[\"Salary\"] == 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c392b-a39b-4d17-9bef-37bca62c737b",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f335d92-eec7-4c6b-b168-debd2f6e85ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition[\"Attrition\"].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb0097e-bbce-40b1-9c3a-8438fb491c93",
   "metadata": {},
   "source": [
    "The data is not imbalanced, we will not need to use techniques like SMOTE or ADASYN, or change the losses of the classification algorithms.\n",
    "\n",
    "However, the target has some missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd9330b-4f05-4381-b7c2-83bbf6519f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_target = df_attrition[\"Attrition\"].isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb2aff-ce75-4ab1-80bc-710793872ab2",
   "metadata": {},
   "source": [
    "## Dealing with missing values and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6270b9-feb4-4119-93c3-aa92fad64501",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_attrition[mask_categorical | mask_numerical | mask_target])/len(df_attrition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ab468-6cb5-430a-a170-c42ff2d1d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop rows with missing values as they only reprsent 1% of the dataset\n",
    "df_attrition = df_attrition[~(mask_categorical | mask_numerical | mask_target)]\n",
    "\n",
    "# We can also drop the Office_Localisation column as it only has one category\n",
    "df_attrition.drop(columns=\"Office_Localisation\", inplace = True)\n",
    "\n",
    "df_attrition.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af99c96c-5f53-4830-9e3c-d63559dc47ce",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9872481-9fc0-4a8d-988b-52b345832a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fix data types\n",
    "df_attrition.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be945264-907f-4459-a6db-d6ffdcdca692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition[\"Average_Monthly_Hours\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a247dc-15b5-46f4-bee7-6b5eaca6640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition[\"Average_Monthly_Hours\"]=df_attrition[\"Average_Monthly_Hours\"].apply(lambda x: int(keep_only_digits(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fdfb7c-a034-478f-9e9a-cdd865796623",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition[\"Average_Monthly_Hours\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df2d80-8bbb-4625-a41d-f5320d56ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition[\"Attrition\"]=df_attrition[\"Attrition\"].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2348e80-da95-4981-abe9-77c68b67fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2b868-75c7-48dc-b32e-d962f3ae18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"Gender\", \"Department\", \"Job_Title\", \"Promotion_Last_5Years\"]\n",
    "numerical_columns = [\"Age\", \"Years_at_Company\", \"Satisfaction_Level\", \"Average_Monthly_Hours\", \"Salary\"]\n",
    "target_column = [\"Attrition\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6a7abe-fada-4961-b9c8-0a375024fa08",
   "metadata": {},
   "source": [
    "## Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b3fb18-3287-4215-a139-0f61c81adbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(categorical_columns):\n",
    "    plt.sca(axes[i])\n",
    "    sns.countplot(data=df_attrition,\n",
    "                 x=col,\n",
    "                 hue=\"Attrition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5634a2b-e806-428f-a452-af61a369b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = df_attrition[numerical_columns].hist(figsize=(10, 8), bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2042def9-e445-45ec-987a-91503f1b1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(20, 5))\n",
    "axes[1,2].remove()\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numerical_columns):\n",
    "    plt.sca(axes[i])\n",
    "    sns.kdeplot(data=df_attrition,\n",
    "                 x=col,\n",
    "                 hue=\"Attrition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1434235d-ec28-4b7b-a11b-dacaf41099b4",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08297902-8fa6-4ae7-b345-9e59b99e67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.heatmap(df_attrition[numerical_columns + target_column].corr(),\n",
    "               annot=True,\n",
    "               cmap=\"Reds\",\n",
    "               fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e422a2-4f5e-4bcb-8115-26c1d36224e5",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78e712-b63d-4d37-b473-d668106e320c",
   "metadata": {},
   "source": [
    "### Numerical Variables binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb4865b-4ee3-4a6c-8ed9-7fbc1be05e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We discretize Age variables into age bins\n",
    "bins = [0, 18, 30, 40, 50, 60, 100]\n",
    "labels = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "df_attrition[\"Age_bin\"]=pd.cut(df_attrition[\"Age\"], bins=bins, labels=labels, right=False)\n",
    "\n",
    "_=sns.countplot(data=df_attrition,\n",
    "                 x=\"Age_bin\",\n",
    "                 hue=\"Attrition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f965de-2f8e-43d3-bee7-1d2ebd5591fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We discretize Salary variable by bins of 10000\n",
    "bins = list(range(30000, 110000, 10000))\n",
    "labels = [i+1 for i in range(len(bins)-1)]\n",
    "\n",
    "df_attrition[\"Salary_bin\"]=pd.cut(df_attrition[\"Salary\"], bins=bins, labels=labels, right=False)\n",
    "\n",
    "_=sns.countplot(data=df_attrition,\n",
    "                 x=\"Salary_bin\",\n",
    "                 hue=\"Attrition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f0587-e7b3-433a-bcc4-c480fb563999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We discretize Satisfaction_Level variable \n",
    "bins = [0, .25, .5, .75, 1]\n",
    "labels = [1, 2, 3, 4]\n",
    "\n",
    "df_attrition[\"Satisfaction_Level_bin\"]=pd.cut(df_attrition[\"Satisfaction_Level\"], bins=bins, labels=labels, right=False)\n",
    "\n",
    "_=sns.countplot(data=df_attrition,\n",
    "                 x=\"Satisfaction_Level_bin\",\n",
    "                 hue=\"Attrition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3053968-2235-4069-8ce2-78388fba2166",
   "metadata": {},
   "source": [
    "### Categorical variables interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece68cb1-1403-4185-ab89-b822b927d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition[\"Department x Job_Title\"] = df_attrition[\"Department\"] + \" - \" + df_attrition[\"Job_Title\"]\n",
    "df_attrition.sort_values(by = \"Department x Job_Title\", ascending = True, inplace = True)\n",
    "_ =  sns.countplot(data=df_attrition,\n",
    "                   x=\"Department x Job_Title\",\n",
    "                   hue=\"Attrition\")\n",
    "_ = plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24757288-c168-4aac-b57e-6f7c5ee179e0",
   "metadata": {},
   "source": [
    "This interaction term brings the same information as a breakdown by Department, it seems like the Department is more important for attrition than job title and interaction term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a0c05e-648b-495e-a7a1-9cae2c81985f",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5807c4e4-7273-4b55-a97a-35abf975a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check if binned features have a better predictive power or not\n",
    "for col in tqdm([\"Age\", \"Satisfaction_Level\", \"Salary\"]):\n",
    "\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_attrition[[f\"{col}\", f\"{col}_bin\"]],\n",
    "                                                       df_attrition[\"Attrition\"],\n",
    "                                                       test_size=.2,\n",
    "                                                       random_state=42)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute importance\n",
    "    importance = permutation_importance(model, X_test, y_test, scoring=\"recall\", n_repeats=100, random_state=42)\n",
    "    \n",
    "    # Print feature importance\n",
    "    feature_importance = pd.DataFrame(\n",
    "        {\"Feature\": [f\"{col}\", f\"{col}_bin\"], \"Importance\": importance.importances_mean}\n",
    "    ).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e6a9e1-6692-4cc5-bd86-1879dddf2e3d",
   "metadata": {},
   "source": [
    "Binned versions of features do not improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cb1fcb-cecc-4b8a-af0e-2e55cf98be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check if the interaction term between Department and Job Title \n",
    "# has a higher feature importance\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        (\"target_encoder\", TargetEncoder(), [\"Department\", \"Job_Title\", \"Department x Job_Title\"])\n",
    "    ]\n",
    ")\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", model)\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_attrition[[\"Department\", \"Job_Title\", \"Department x Job_Title\"]],\n",
    "                                                   df_attrition[\"Attrition\"],\n",
    "                                                   test_size=.2,\n",
    "                                                   random_state=42)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Compute importance\n",
    "importance = permutation_importance(pipeline, X_test, y_test, scoring=\"recall\", n_repeats=100, random_state=42)\n",
    "\n",
    "# Print feature importance\n",
    "feature_importance = pd.DataFrame(\n",
    "    {\"Feature\": [\"Department\", \"Job_Title\", \"Department x Job_Title\"], \n",
    "     \"Importance\": importance.importances_mean}\n",
    ").sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e99d8a-67d1-48f3-80a6-5ecb0a6ead98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition.drop(columns=[\"Age_bin\", \"Salary_bin\", \"Satisfaction_Level_bin\", \"Department x Job_Title\", \"Job_Title\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a8ec8e-1ab1-4df3-81ba-9e5921ef6fb1",
   "metadata": {},
   "source": [
    "# Classification pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4efde-34c7-4d32-97e7-6450551ec5c0",
   "metadata": {},
   "source": [
    "## Model Selection by Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb6d48d-128a-4421-a2b7-6d7d8a920f91",
   "metadata": {},
   "source": [
    "- For Gender variable we use One-Hot Encoding\n",
    "- For Department and Job_Title we use Target Encoding as those variables present several categories\n",
    "- We use normalization on numerical features and taregt encoded features, but not one binary features (Gender after One-Hot Encoding and Promotion_Last_5Years)\n",
    "- Label Encoding and Normalization are made inside the Cross Validation for each train-test split to avoid data leakage\n",
    "- We compare models on the recall as the recall is the metrics of interest here : we want to minimize the number of False Negatives as we don't want to predict people who will leave as people who want to stay, because in the case of someone leaving without predicting it the cost is high for the company\n",
    "- Promotion_Last_5Years variable is already One Hot Encoded, it is not modified then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1cf1c3-ee2c-401d-89de-c33fcf16d5e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We create first a pipeline that target encodes Department and Job_Title\n",
    "# variables and then scales the numerical variables created\n",
    "pipeline_target_encoder = make_pipeline(\n",
    "    TargetEncoder(),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "# We create a preprocessing pipeline for all variables to use in Cross Validation\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        (\"scaler\", StandardScaler(), [\"Age\", \"Years_at_Company\", \"Average_Monthly_Hours\", \"Salary\"]),\n",
    "        (\"target_encoder\", pipeline_target_encoder, [\"Department\"]),\n",
    "        (\"one_hot_encoder\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), [\"Gender\"]),\n",
    "        (\"passthrough\", \"passthrough\", [\"Promotion_Last_5Years\", \"Satisfaction_Level\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# We define classifiers\n",
    "dict_classifiers = {\n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Ridge Regression\":LogisticRegression(penalty='l2', solver='lbfgs', C=1.0),\n",
    "    \"Lasso Regression\":LogisticRegression(penalty='l1', solver='liblinear', C=1.0),\n",
    "    \"ElasticNet Regression\":LogisticRegression(penalty='elasticnet', solver='saga', C=1.0, l1_ratio=0.5),\n",
    "    \"Random Forest\":RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Linear Discriminant Analysis\": LinearDiscriminantAnalysis(),\n",
    "    \"Linear SVM\": SVC(kernel=\"linear\", probability=True),\n",
    "    \"RBF SVM\":SVC(kernel=\"rbf\", probability=True),\n",
    "    \"Gradient Boosting\":GradientBoostingClassifier(),\n",
    "    \"XGB\":XGBClassifier(),\n",
    "    \"LGBM\":LGBMClassifier(verbose=-1),\n",
    "    \"MLP\":MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500),\n",
    "    \"Gaussian Naive Bayes\":GaussianNB(),\n",
    "    \"Bernoulli Naive Bayes\":BernoulliNB(),\n",
    "    \"Bagging\":BaggingClassifier(),\n",
    "    \"Voting\":VotingClassifier(estimators=[('rf', RandomForestClassifier()), ('xgb', XGBClassifier())], voting='soft'),\n",
    "    \"KNN\":KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Quadratic Discriminant Analysis\":QuadraticDiscriminantAnalysis()\n",
    "}\n",
    "\n",
    "dict_recalls = {}\n",
    "\n",
    "for name, classifier in tqdm(dict_classifiers.items()):\n",
    "    \n",
    "    # We create the final pipeline (preprocessing + classifier)\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", classifier)\n",
    "    ])\n",
    "    \n",
    "    # We apply cross validation and compute mean recall\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    scores = cross_val_score(pipeline,\n",
    "                            df_attrition.drop(columns=[\"Attrition\"]),\n",
    "                            df_attrition[\"Attrition\"],\n",
    "                            cv=cv,\n",
    "                            scoring=\"recall\")\n",
    "\n",
    "    dict_recalls[name]=float(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3e5a0-6624-4345-ba5f-6e511104590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(sorted(dict_recalls.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e224d02d-e015-426d-b9ad-1292d5668690",
   "metadata": {},
   "source": [
    "Linear SVM gives the best score, we will fine tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aafcee-fdaa-49d1-bec6-963b7ba71007",
   "metadata": {},
   "source": [
    "## Fine tuning of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c24dded-bc38-412e-931b-9a942c2b3ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create first a pipeline that target encodes Department and Job_Title variables and then scales the numerical variables created\n",
    "pipeline_target_encoder = make_pipeline(\n",
    "    TargetEncoder(),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "# We create a preprocessing pipeline for all variables to use in Cross Validation\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        (\"scaler\", StandardScaler(), [\"Age\", \"Years_at_Company\", \"Average_Monthly_Hours\", \"Salary\"]),\n",
    "        (\"target_encoder\", pipeline_target_encoder, [\"Department\"]),\n",
    "        (\"one_hot_encoder\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), [\"Gender\"]),\n",
    "        (\"passthrough\", \"passthrough\", [\"Promotion_Last_5Years\", \"Satisfaction_Level\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the Linear SVM model\n",
    "svm_model = SVC(kernel=\"linear\", probability=True, random_state=42)\n",
    "\n",
    "# Define the parameter grid for tuning SVM\n",
    "param_grid = {\n",
    "    'classifier__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'classifier__max_iter': [-1]  # Maximum number of iterations for convergence\n",
    "}\n",
    "\n",
    "# Create the final pipeline (preprocessing + classifier)\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", svm_model)\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=StratifiedKFold(n_splits=5, shuffle=True), scoring=\"recall\")\n",
    "\n",
    "# Fit the model with grid search\n",
    "grid_search.fit(df_attrition.drop(columns=[\"Attrition\"]), df_attrition[\"Attrition\"])\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Recall Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Optionally, you can retrieve the best estimator and use it for prediction:\n",
    "best_svm_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e08ac-55ea-4c21-8c0c-3652e5412750",
   "metadata": {},
   "source": [
    "Best model is linear SVM with c=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0721eac8-6b98-4ae5-8e9e-0062427f329c",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bc8de-7372-4ecb-aa36-a3e1a196cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938af0f-2cc6-4941-bb39-2de133394e8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We train test split the data then use the pipeline (preprocessors + linear SVM with C=0.1)\n",
    "# found earlier and evaluate the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_attrition.drop(columns=\"Attrition\"),\n",
    "                                                   df_attrition[\"Attrition\"],\n",
    "                                                   test_size=.2,\n",
    "                                                   random_state=42)\n",
    "\n",
    "\n",
    "best_svm_model.fit(X_train, y_train)\n",
    "y_pred = best_svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9347f7-bf3a-4992-928b-56eb5a650120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make the evaluation\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(f\"Recall score : {recall_score(y_test, y_pred):.2f}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Accuracy score : {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad568c-ed8f-4a0a-aada-df3bc4a21ee1",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55e953-aa12-4ac5-8f30-721dea58d983",
   "metadata": {},
   "source": [
    "We use permutation feature importance and SHAP, two feature importance measures that are model agnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8796865-6c3c-473b-8e37-50bb07664d1a",
   "metadata": {},
   "source": [
    "### Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e4516c-3a7b-4866-8f06-cf5c33d1d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute permutation importance\n",
    "perm_importance = permutation_importance(best_svm_model, \n",
    "                                         X_test, \n",
    "                                         y_test, \n",
    "                                         scoring=\"recall\", \n",
    "                                         n_repeats=10, \n",
    "                                         random_state=42)\n",
    "\n",
    "# Create DataFrame\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X_test.columns,\n",
    "    \"Importance\": perm_importance.importances_mean\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "perm_importance_df.set_index(\"Feature\").plot(kind=\"bar\")\n",
    "_ = plt.axhline(0, color='black', linewidth=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b00d60-99fd-4d73-8a3d-d477e7788bf3",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbbcc4b-3075-4a04-b7ac-64c74cef92a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the preprocessor step from the pipeline and transform X_test\n",
    "preprocessor = best_svm_model.named_steps[\"preprocessor\"]\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "feature_names = best_svm_model.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "\n",
    "# Use KernelExplainer for SHAP (since SVM is not tree-based)\n",
    "explainer = shap.KernelExplainer(best_svm_model.named_steps[\"classifier\"].predict_proba, X_test_transformed)\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer.shap_values(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cf0c81-888f-43bb-b2b6-0f6e8f12bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e53234-22e8-4b36-bf79-a026d121a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e5aded-2743-4dd7-99fb-3d3702d31ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert SHAP values to a DataFrame for plotting\n",
    "shap_importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Shapley Importance\": np.mean(np.abs(shap_values[..., 1]), axis=0)  # Absolute importance for positive class\n",
    "}).sort_values(by=\"Shapley Importance\", ascending=False)\n",
    "\n",
    "# Plot the feature importance\n",
    "shap_importance_df.set_index(\"Feature\").plot(kind=\"bar\", figsize=(10, 5))\n",
    "plt.axhline(0, color=\"black\", linewidth=0.5)\n",
    "plt.title(\"Feature Importance using SHAP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a0271-657a-4bc8-ae26-64b3ed2bd83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot\n",
    "shap.summary_plot(shap_values[..., 1], X_test_transformed, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebace3b-399c-4349-a932-031eca6442fe",
   "metadata": {},
   "source": [
    "## Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80b01e-6263-4733-9325-b48589523527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "joblib.dump(best_svm_model, \"attrition_classification_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
